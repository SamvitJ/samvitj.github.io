---
layout: post
title: "On Computer Science"
date: 2017-09-15
comments: true
---

I wrote my first computer programs in 6th grade for a school “science” fair (anything remotely scientific qualified) - a C program to convert temperatures from Celsius to Fahrenheit (and back), and a C program to solve a quadratic equation, given its coefficients.

I was not impressed. I thought programming was dull and artificial. When a computer could go from bits and bytes in silicon to printing English text on my monitor, a seemingly incredible feat, why did we need humans to write code to teach it how to use arithmetic operations (1st grade math) to compute algebraic functions (8th grade math)?

I gave programming a second shot in the summer before 9th grade. I made more progress this time - learning the C language to a commendable extent, pointers and all - but was bored to death by the process. So much effort to pick up a dry, formal human artifact, a mere tool for expressing computation. In my mind, the ideas worth learning were the unchanging ones, ideas like the wave-particle duality of light and the fundamental conservation laws - beautiful, physical truths about our universe, expressed in mathematics, that would exist even if no humans discovered them.[^1]

My culminating project that summer was a Sudoku solver. Given a valid initialization of the Sudoku grid, my program could use the two or three basic heuristics I knew to output a solution to easy puzzles. Intrigued but not fascinated, I put programming on the back-burner once again.

The breakthrough happened in the summer after my freshman year of college, in 2014. The previous summer, I had learnt the basics of making apps for my iPhone 4, my first smartphone. I loved music, and I loved sharing it with friends and family - so I began work on an app to make sharing links fast, simple, and personal.

2014 was the height of [social-local-mobile](../assets/links/startups-social-local-mobile.pdf). The idea that two Stanford students could build something for their phone in their dorm that was almost as valuable as a car company or an oil company struck me as bizarre and incredible. I was joining a revolution.

I think my biggest shock, and probably the biggest shock for anyone who invents something or builds something new for the first time, no matter how humble, is that it worked.

I had a vision for what this app would do, and after a month of coding, I was using it - my family was using it, it was real.

There was instant gratification. I had an idea for a feature - say, forwarding links - and after half a day of hacking in Xcode, I had it in my hands.

For the first time, programming blew my mind. That with code I could build a module on my phone that could access anything on the Internet - from YouTube to my private web server - and that could communicate with a clone of itself installed on another phone in another country, simply by reading and writing messages to a database somewhere in the Amazon cloud - was almost as exciting to me as the idea that the same theory that explains nuclear power explains the Sun and stars and the mechanics of space and time.

I had felt some angst freshman year. Why was I studying computer science? I liked using mathematical models to explain real phenomena - an idea that underlay my interest in physics in high school, and my interest in economics early in college. What was computer science about? Why were computers, a human creation, worth studying?

For me, that summer with LinkMeUp was the turning point. At the risk of sounding a little cliché, I knew at that point what I wanted to do for the rest of my life, and I haven’t looked back since.

I’m now a grad student studying machine learning and databases, and I’ll still admit that computer science, even at its deepest, isn’t as profound or as beautiful as physics.

But fascination is not what draws me to the field (though globally synchronized, distributed databases *are* pretty cool). It’s a more mature appreciation that it is computer scientists who helped put man on the moon, placed the world’s collective knowledge in a small device in my pocket, and put two billion people and counting on a previously faceless Internet. It's an understanding that it is *computer scientists* - inventors and engineers, not thinkers or writers - who will tackle the three great challenges of the 21st century: reversing the impact of two centuries of industrialization, diagnosing disease and eliminating it from the human genome, and recreating, or maybe even surpassing, human intelligence in silicon and code.

<!-- I often wonder why I didn't start programming seriously earlier. There were two times when it could have taken off for me, after all. Part of the answer is obviously personal interests and aptitude. But another reason is that, as a kid born in the mid-90s, I took computers for granted. I was typing away on my Windows 98 in 1st grade,
having never known any other world.

But getting my first iPhone was different. I remember my dad bringing home an early smartphone from a business trip to Japan, when I was in elementary school. I saw the evolution from flip phones to touchscreens to iPhones happen in front of my eyes. My revolution was the mobile revolution, a revolution in which hundreds of millions of people who would never own a personal computer suddenly had Internet access in their hands. -->

### Footnotes

[^1]: This is a simplification: physics is not completely unchanging, nor wholly divorced from human thought, either. One way to understand the evolution of physics, actually, is as a series of better and better approximations to the behavior of the real world. New approximations, such as relativity or quantum mechanics, are triggered by the discovery of new edge cases that can't be explained by previous theories.<!--</br></br>
At the same time, the core laws of physics are more than macroscopic, empirical observations. To make this distinction clear, consider the following example: the fact that static friction is proportional to the weight vector of an object is not a core law of physics, but an empirical heuristic, an aggregate statement about the effect of electrostatic forces at the boundary between the object and the supporting surface. In contrast, Newton's second law, $F = ma$, is a core physical law, that if true, would apply equally to large objects (e.g. a baseball) as it would to subatomic particles (e.g. electrons). The distinctive characteristic of such laws is that they can explain even basic interactions between the most fundamental components of a system, such as the acceleration of an electron in the vicinity of a proton.</br></br>
Note also that relativity doesn't completely overthrow Newton's laws, but introduces subtle corrections to variables we might naively assume to be constants, such as mass. This is discussed in more detail in the next footnote.-->

[^2]: $E = mc^2$, which explains the "hidden" energy released in nuclear fission and fusion, the reactions that power nuclear plants and all the universe's stars, respectively, is a consequence of a relativistic correction to the $m$ in $F = ma$. This correction is derived from the fact that momentum is conserved in all reference frames, even when relativity is taken into account. In particular, relativity predicts that measurements of time and length are dependent on the relative velocity of reference frames, which forces a redefinition of mass in terms of an object's velocity to preserve momentum conservation. These changes in our measurement of time and space, termed time dilation and length contraction, respectively, are themselves a consequence of the fact that the speed of light is the same in all reference frames, regardless of one's velocity relative to the source. This is special relativity's core and perhaps most startling prediction, a direct result of two ideas: 1) the central axiom of relativity, which states that the laws of physics are the same in all uniformly moving reference frames, and 2) the fact that Maxwell's equations, valid laws of physics, predict a specific value for the speed of light.
