<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<title>A Brief Primer: Stochastic Gradient Descent | Samvit Jain</title>
	<link rel="stylesheet" href="http://www.samvitjain.com/blog/css/text.css" type="text/css" media="screen, projection" />
	<link rel="stylesheet" href="../css/text.css" type="text/css" media="screen, projection" />
	<link rel="stylesheet" href="http://www.samvitjain.com/blog/css/style.css" type="text/css" media="screen, projection" />
	<link rel="stylesheet" href="../css/style.css" type="text/css" media="screen, projection" />
	
</head>
<body>
  <div id="header">
    <h1><a href="http://www.samvitjain.com">Samvit Jain</a>'s <a href="http://www.samvitjain.com/blog">blog</a></h1>
<span> </span>
  </div>
  <hr />
<div class="content">
    <div class="post">

<h2 class="post-title"><a href="http://www.samvitjain.com/blog/gradient-descent/">A Brief Primer: Stochastic Gradient Descent</a></h2>

<p class="date">20 Jul 2017</p>

<p><em>&quot;Nearly all of deep learning is powered by one very important algorithm: stochastic gradient descent.&quot; - Ian Goodfellow</em></p>
<p>Many machine learning papers reference various flavors of stochastic gradient descent (SGD) - <a href="http://martin.zinkevich.org/publications/nips2010.pdf">parallel</a> SGD, <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf">asynchronous</a> SGD, <a href="https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf">lock-free parallel</a> SGD, and even <a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf?">distributed synchronous</a> SGD, to name a few.</p>
<p>This topic isn't just a theoretical curiosity. Parallelizing important optimization algorithms, like SGD, is the key to fast training and, by extension, fast model development. Not surprisingly, three out of the four papers I just referenced came out of industry research.</p>
<p>To orient a discussion of these papers, I thought it would be useful to dedicate one blog post to briefly developing stochastic gradient descent from &quot;first principles.&quot; This discussion is supposed to be illustrative, and errs in favor of pedagogical clarity, over mathematical rigor.</p>
<h3 id="optimization">Optimization</h3>
<p>Stochastic gradient descent is an optimization algorithm. In machine learning, what exactly does an optimization algorithm optimize?</p>
<p>As you may know, supervised machine learning generally consists of two phases:<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> training (generating a model) and inference (making predictions with that model). Training involves finding values for a model's <em>parameters</em>, <span class="math inline"><em>θ</em></span>, such that two, often conflicting, goals are met: 1) error on the set of training examples is minimized, and 2) the model generalizes to new data.</p>
<p>Optimization algorithms are the means used to find these <em>optimal</em> parameters, and develop robust models. Some examples of optimization algorithms include gradient descent, the conjugate gradient method, BFGS, and L-BFGS.</p>
<h3 id="from-the-beginning">From the beginning</h3>
<p>Let's begin with stochastic gradient descent's predecessor and cousin: <strong>gradient descent</strong>. Gradient descent is an algorithm that iteratively tweaks a model's parameters, with the goal of minimizing the discrepancy between the model's predictions and the &quot;true&quot; labels associated with a set of training examples.</p>
<p>This discrepancy is commonly termed &quot;cost&quot; or &quot;loss&quot; and is quantified in a <em>cost function</em>. Here's a common cost function used with gradient descent: <br /><span class="math display">$$
\begin{aligned}
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2
\end{aligned}
$$</span><br /></p>
<p>Let's unpack this. Here \(m\) is the number of training examples, \(x^{(i)}\) is the \(i\)th training example, \(y^{(i)}\) is the \(i\)th corresponding label, and <span class="math inline"><em>θ</em></span> is a vector representation of our model's parameters. So <span class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>i</em>)</sup>)</span> is the prediction our model <span class="math inline"><em>h</em><sub><em>θ</em></sub></span> makes on example \(x^{(i)}\).</p>
<p>Then <span class="math inline">(<em>h</em><sub><em>θ</em></sub>(<em>x</em><sup>(<em>i</em>)</sup>)−<em>y</em><sup>(<em>i</em>)</sup>)<sup>2</sup></span> is the square of the difference between our model's prediction and the actual label for the \(i\)th training example. We sum over all the training examples, and divide by \(m\) to compute the average, squared-error.</p>
<p>So is <span class="math inline"><em>J</em>(<em>θ</em>)</span> just one-half of the <em>mean-squared error</em> on the training examples.</p>
<p>Note that we could aim to minimize any multiple of the mean-squared error, and we'd still be minimizing the actual mean-squared error in the process. Thus, it's reasonable to take <span class="math inline"><em>J</em>(<em>θ</em>)</span> as our particular cost function.</p>
<p>Now back to gradient descent. At each iteration, we perform an update operation on our parameter vector <span class="math inline"><em>θ</em></span>. The full algorithm:</p>
<p>    <span style="font-variant: small-caps;">Gradient Descent</span> <br /><span class="math display">$$
\begin{aligned}
&amp; \text{repeat until convergence } \{ \\
&amp; \quad \theta \leftarrow \theta - \gamma \nabla J(\theta) \\
&amp; \}
\end{aligned}
$$</span><br /></p>
<p>Here is <span class="math inline"><em>γ</em></span> is the learning rate, or step-size. Recall that <span class="math inline">∇<em>J</em>(<em>θ</em>)</span> is the vector derivative of <span class="math inline"><em>J</em>(<em>θ</em>)</span>, and points in the direction in which <span class="math inline"><em>J</em>(<em>θ</em>)</span> rises most rapidly. So to minimize <span class="math inline"><em>J</em>(<em>θ</em>)</span>, we simply take a step of size <span class="math inline"><em>γ</em></span> in the exact opposite direction.</p>
<p>We do this repeatedly until we arrive at a point where <span class="math inline">∇<em>J</em>(<em>θ</em>)=0</span>. This represents a local or global minimum of our cost function <span class="math inline"><em>J</em>(<em>θ</em>)</span>. At such a point, we say gradient descent has <em>converged</em>. Note that if <span class="math inline"><em>J</em>(<em>θ</em>)</span> has certain properties, notably convexity, and <span class="math inline"><em>γ</em></span> is properly tuned, gradient descent will in fact converge at the global minimum.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<figure>
<img src="../assets/gradient-descent/gradient-descent.png" alt="Figure 1: An illustration of gradient descent. Note that, at each iteration, the algorithm moves the black marker (a representation of the parameter vector \theta) in the direction of steepest descent. This process continues until the marker arrives at a local minimum. Note also how influential the initial value of \theta is to the final outcome in this particular, non-convex gradient map. On such a terrain, a slight initial perturbation can lead to convergence at an entirely different minimum point. (Source: Machine Learning | Coursera)" style="width:100.0%" /><figcaption><sup><strong>Figure 1</strong>: An illustration of gradient descent. Note that, at each iteration, the algorithm moves the black marker (a representation of the parameter vector <span class="math inline"><em>θ</em></span>) in the direction of <em>steepest descent</em>. This process continues until the marker arrives at a local minimum. Note also how influential the initial value of <span class="math inline"><em>θ</em></span> is to the final outcome in this particular, non-convex gradient map. On such a terrain, a slight initial perturbation can lead to convergence at an entirely different minimum point. (Source: <a href="https://www.coursera.org/learn/machine-learning/">Machine Learning | Coursera</a>)</sup></figcaption>
</figure>
<p>To be totally clear, the update operation <span class="math inline"><em>θ</em> − <em>γ</em>∇<em>J</em>(<em>θ</em>)</span> is a <em>vector subtraction</em>. Both the parameter vector, <span class="math inline"><em>θ</em></span>, and the gradient, <span class="math inline">∇<em>J</em>(<em>θ</em>)</span>, are \(n\)-dimensional vectors, where \(n\) denotes the number of features in our model. So every update operation in gradient descent is performed on the \(n\)-dimensional feature space of our model.</p>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>Gradient descent has a problem. Computing the gradient of the cost function, <span class="math inline">∇<em>J</em>(<em>θ</em>)</span>, is very costly in practice. Recall <br /><span class="math display">$$
\begin{aligned}
\nabla J(\theta) = (\frac{\partial}{\partial \theta_1} J(\theta), \frac{\partial}{\partial \theta_2} J(\theta), ..., \frac{\partial}{\partial \theta_n} J(\theta))
\end{aligned}
$$</span><br /></p>
<p>where the subscripts represent an index on features. Each partial derivative in this vector involves computing a sum over <em>every training example</em>: <br /><span class="math display">$$
\begin{aligned}
\frac{\partial}{\partial \theta_j} J(\theta) 
&amp;= \frac{\partial}{\partial \theta_j} \left( \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2 \right) \\
&amp;= \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial \theta_j} h_{\theta}(x^{(i)})
\end{aligned}
$$</span><br /></p>
<p>(Note that we are using a different index, \(j\), for the partial derivatives, as they are taken with respect to the model's \(n\) features. The index \(i\) refers to the training examples.)</p>
<p>For large training sets (as most are), this is prohibitively expensive. The key idea in stochastic gradient descent is to drop the sum, and use the following as a very rough proxy for our partial derivative: <br /><span class="math display">$$
\begin{aligned}
\frac{\partial}{\partial \theta_j} J(\theta) \approx (h_{\theta}(x^{(i)}) - y^{(i)}) \frac{\partial}{\partial \theta_j} h_{\theta}(x^{(i)})
\end{aligned}
$$</span><br /> where <span class="math inline">(<em>x</em><sup>(<em>i</em>)</sup>, <em>y</em><sup>(<em>i</em>)</sup>)</span> is any one particular training example.</p>
<p>Then, in stochastic gradient descent, instead of summing over every training example at each step, and iterating until convergence, we cycle through the training examples in random order, using a <em>single</em> one at each iteration in our cost function:</p>
<p>    <span style="font-variant: small-caps;">Stochastic Gradient Descent</span> <br /><span class="math display">$$
\begin{aligned}
&amp; \text{repeat until convergence } \{ \\
&amp; \quad \text{for } i := 1, 2, ..., m \space \{ \\
&amp; \quad \quad \theta \leftarrow \theta - \gamma \nabla J(\theta; x^{(i)}; y^{(i)}) \\
&amp; \quad \} \\
&amp; \}
\end{aligned}
$$</span><br /></p>
<p>where we have, from above: <br /><span class="math display">$$
\begin{aligned}
\nabla J(\theta; x^{(i)}; y^{(i)}) = (h_{\theta}(x^{(i)}) - y^{(i)}) \nabla h_{\theta}(x^{(i)})
\end{aligned}
$$</span><br /></p>
<p>Note that, in practice, the number of times we have to run the inner loop depends on the size of the training set. If the number of training examples is huge, a single iteration through the training examples can suffice. Most data sets require between 1 to 10 runs though the inner loop.</p>
<figure>
<img src="../assets/gradient-descent/comparison.png" alt="Figure 2: Unlike in gradient descent, the value of the cost function does not necessarily decrease with each iteration in stochastic gradient descent. Even if the error on one particular training example is reduced, it is possible (and in the beginning, almost as likely) that the error on the entire training set will increase. With proper tuning of the learning rate, however, stochastic gradient descent will approach the same minimum as gradient descent. (Source: BogoToBogo)" style="width:80.0%" /><figcaption><sup><strong>Figure 2</strong>: Unlike in gradient descent, the value of the cost function does not necessarily decrease with each iteration in stochastic gradient descent. Even if the error on one particular training example is reduced, it is possible (and in the beginning, almost as likely) that the error on the <em>entire</em> training set will increase. With proper tuning of the learning rate, however, stochastic gradient descent will approach the same minimum as gradient descent. (Source: <a href="http://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/stochastic-vs-batch-gradient-descent.png">BogoToBogo</a>)</sup></figcaption>
</figure>
<p>Though it may not be immediately obvious, theory assures us that if the learning rate <span class="math inline"><em>γ</em></span> is reduced appropriately over time, and the cost function satisfies certain properties (i.e. convexity), stochastic gradient descent will <em>also</em> converge.</p>
<p>Finally, it is worth noting that there is a middle-ground between gradient descent and stochastic gradient descent, called mini-batch gradient descent. Mini-batch gradient descent uses a randomly selected subset, or <em>mini-batch</em>, of \(b\) training examples at each iteration, instead of just one. Some definitions of SGD actually refer to mini-batch gradient descent. In practice, batching can lead to a more stable trajectory than in SGD, and, perhaps surprisingly, better performance as well, given that the gradient computation is properly vectorized.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<h3 id="parallelization">Parallelization</h3>
<p>Note that stochastic gradient descent, as we have described it thus far, is a sequential algorithm. Each update operation involves both 1) reading the current parameter vector <span class="math inline"><em>θ</em></span>, and 2) writing to <span class="math inline"><em>θ</em></span> a modified value. So we cannot just naively execute a series of update operations in parallel.</p>
<p>How would we go about parallelizing SGD? One could imagine an approach in which we take snapshot reads of the parameter vector, and periodically merge updates based on a given read in a synchronization step.</p>
<figure>
<img src="../assets/gradient-descent/parallel-sync.png" alt="Figure 3: Synchronous parallel SGD (Source: Stanford)" style="width:50.0%" /><figcaption><sup><strong>Figure 3</strong>: Synchronous parallel SGD (Source: <a href="http://stanford.edu/~imit/tuneyourmomentum/theory/">Stanford</a>)</sup></figcaption>
</figure>
<p>Alternatively, we could get rid of synchronization altogether, and have worker threads (or processes) read the parameter vector from a shared memory bus, compute a gradient, and then send back an update to a master. It would then be the master node's responsibility to incorporate the updates in some total ordering, while sensibly handling concurrent writes (e.g. via locks).</p>
<figure>
<img src="../assets/gradient-descent/parallel-async.png" alt="Figure 4: Asynchronous parallel SGD (Source: Stanford)" style="width:50.0%" /><figcaption><sup><strong>Figure 4</strong>: Asynchronous parallel SGD (Source: <a href="http://stanford.edu/~imit/tuneyourmomentum/theory/">Stanford</a>)</sup></figcaption>
</figure>
<p>Is there any guarantee that SGD with version reconciliation would converge? What would be the impact on performance of explicit synchronization (approach 1) and arbitrary locking (approach 2)?</p>
<p>These are the questions that the literature on parallelizing stochastic gradient descent seeks to answer.</p>
<p><em>Thanks to Kiran Vodrahalli, Prem Nair, and Sanjay Jain for reviewing drafts of this post.</em></p>
<h3 id="footnotes">Footnotes</h3>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There is also an important intermediate stage: validation, used to determine the values of our model's hyperparameters. Hyperparameters are meta-parameters that dictate how a particular model is constructed. In gradient descent, the learning rate <span class="math inline"><em>γ</em></span> is a key hyperparameter.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>A function is convex if a line segment between any two points on its graph lies above or on the graph. An upward-facing parabola is a simple example of a convex function. The problem of minimizing convex functions, or <a href="https://en.wikipedia.org/wiki/Convex_optimization">convex optimization</a>, is an entire subfield of theoretical machine learning. See <a href="https://www.cs.cmu.edu/~ggordon/10725-F12/slides/05-gd-revisited.pdf">these slides</a> for a brief proof sketch that gradient descent converges on convex functions.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Assuming the appropriate vectorization libraries are present, it may be possible to compute the following batch gradient in parallel on a multi-core machine: <br /><span class="math display">$$
\begin{aligned}
\nabla J(B) = \frac{1}{|B|} \sum_{i=1}^{|B|} (h_{\theta}(x^{(i)}) - y^{(i)}) \nabla h_{\theta}(x^{(i)})
\end{aligned}
$$</span><br /> In a <a href="https://research.fb.com/publications/ImageNet1kIn1h/">recent paper</a>, Facebook AI Research took this idea to the extreme, training ImageNet in 1 hour with a mini-batch size of 8192 images on 256 GPUs.<a href="#fnref3">↩</a></p></li>
</ol>
</section>



<div id="follow"><i>You can follow me on
Twitter <a href="https://twitter.com/Samvit_Jain">here</a>.</i></div>

<hr />

<div id="disqus_thread"></div>
<script>
/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/* var disqus_config = function () {
	this.page.url = "http://www.samvitjain.com/blog/gradient-descent/"; // Replace PAGE_URL with your page's canonical URL variable
	this.page.identifier = "/gradient-descent"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
	var d = document, s = d.createElement('script');
	s.src = 'https://samvit-jain.disqus.com/embed.js';
	s.setAttribute('data-timestamp', +new Date());
	(d.head || d.body).appendChild(s);
})(); */
</script>
<!-- <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript> -->

<hr />

<div id="call-to-action">
	<h3>Read More</h3>
	<ul>

	<li>
		<a href="http://www.samvitjain.com/blog/links/">Samvit's Guide to the World Wide Web</a>
		(28 Aug 2017)
	</li>
	
	<li>
		<a href="http://www.samvitjain.com/blog/evaluating-startups-2/">How to Pick Your Next Gig: Evaluating Startups - Part II</a>
		(14 Aug 2017)
	</li>
	
	<li>
		<a href="http://www.samvitjain.com/blog/evaluating-startups/">How to Pick Your Next Gig: Evaluating Startups - Part I</a>
		(14 Aug 2017)
	</li>
	
	<li>
		<a href="http://www.samvitjain.com/blog/massively-parallel-methods/">Why Parallelism? An Example from Deep Reinforcement Learning</a>
		(06 Jul 2017)
	</li>
	
	</ul>
</div>

<footer>
  Blog <a href="http://www.bailis.org">design credit</a>
</footer>

</div>




</div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-102797768-1', 'auto');
  ga('send', 'pageview');
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/javascript">
 MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['\\(','\\)']],
    processEscapes: true
  }
});</script>
</body>
</html>
